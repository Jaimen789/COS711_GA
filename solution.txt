-> Data Analysis
    - charts + plots
    - distribution of malaria in the dataset
    
-> Data Preprocessing
    -> categorical encoding : Each class label (e.g., NEG, Trophozoite, WBC) is mapped to an integer value, starting from 0
        => NEG: 0 , Trophozoite: 1 , WBC: 2
    -> split into train/validation/test
    -> sampling : stratified sampling (maintain even proportion as compared to origanl dataset)


-> Architecture
    => Mini-batch training
    => Model : Adam (Momentum based NN)
        -> Input:
            - Batch Size
            - Height of image
            - Width of image
            - three color channels (RGB)

            The neural network input consists of images resized to 224 Ã— 224 pixels, normalized to a [0, 1] range, \
            with three color channels (RGB).

        -> loss function : categorical_crossentropy
        -> metric of evaluation : accuracy
        -> Activation function : 
            - ReLu us applied at all layers except the output layer
            - SoftMax applied at output layer

        -> Hyperparameters :
            -> Batch Size
            -> Learning rate
            -> Number of Filters in Conv2D Layers: 32, 64, 64 in the Conv2D layers specify the number of filters to 
                learn at each layer. These determine how many feature maps will be generated by each convolutional layer.

            -> Kernel Size: (3, 3) in the Conv2D layers specifies the size of the convolutional kernel. 
                This defines how many pixels the kernel will cover during the convolution operation.

            -> Pooling Size: ndicates the pooling size, which reduces the spatial dimensions of the feature maps by 
                taking the maximum value over a 2x2 window.

            -> Pooling (We are using Max pooling. Add explnation for max pooling): Pooling is a downsampling operation used in Convolutional Neural Networks (CNNs) 
            to reduce the spatial dimensions (height and width) of the input feature maps. This process helps decrease the number of parameters, reduce computation, 
            and control overfitting while retaining the important information from the features.
                    - Dimensionality Reduction
                    - Translation invariance
                    - Reduce of overfitting

        The Convolutional Neural Network (CNN) begins with an input layer accepting RGB images of size 224x224x3 pixels. 
        The first convolutional layer applies 32 filters of size 3x3, followed by ReLU activation to introduce non-linearity, 
        and then a 2x2 max pooling layer that reduces the spatial dimensions by half. The second layer deepens the network with 
        64 3x3 filters, again followed by ReLU and another 2x2 max pooling operation. A third convolutional block with 64 more 3x3 filters, 
        ReLU activation, and max pooling further processes the features. After the convolutional sections, the network flattens the 3D feature maps 
        into a 1D vector, which feeds into a dense layer with 64 neurons and ReLU activation. To prevent overfitting, a dropout layer is added 
        with a 50% dropout rate, randomly deactivating half the neurons during training. Finally, the output layer consists of 3 neurons with softmax activation, 
        corresponding to your three classes, producing probability distributions that sum to 1. The model uses the Adam optimizer and 
        categorical crossentropy loss function, which is appropriate for multi-class classification tasks, while tracking accuracy as the performance metric.



-> Early stopping if the validation loss does not improve for 5 epochs


Analysis of Results:
	- final accuracy + charts
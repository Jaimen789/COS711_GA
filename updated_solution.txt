-> Data Analysis
    - charts + plots
    - distribution of malaria in the dataset
    
-> Data Preprocessing
    -> categorical encoding : Each class label (e.g., NEG, Trophozoite, WBC) is mapped to an integer value, starting from 0
        => NEG: 0 , Trophozoite: 1 , WBC: 2
    -> split into train/validation/test
    -> sampling : stratified sampling (maintain even proportion as compared to origanl dataset)

-> Data Augmentation
    - Rescale
    - Rotation Range : Randomly rotates images within a range of 20 degrees
    - Zoom Range : Applies a random zoom of up to 20%.
    - Horizontal Flip : randomly flips images horizontally.
    - Width Shift: Shifts the image horizontally by up to 20% of the image width.
    - Height shift: hifts the image vertically by up to 20% of the image height.
    - Shear Range: Applies a shearing transformation of up to 20%.
    - Fill Mode: Fills in any newly created pixels after a transformation with the nearest pixel value.

-> Regularization techniques:
    - Dropout: In the model architecture, Dropout layers with rates of 0.5 and 0.3 are included before the dense layers. 
                Dropout is a common regularization method that randomly deactivates a percentage of neurons during each 
                training iteration, which helps prevent the model from relying too heavily on specific neurons, thus reducing overfitting.

    - Early Stopping: The EarlyStopping callback monitors validation loss, stopping training if there is no improvement after a set number 
                    of epochs. This prevents the model from overfitting by stopping training once the model’s performance on the validation 
                    set stops improving.

    - ReduceLROnPlateau: This callback reduces the learning rate when a metric (here, validation loss) has stopped improving. By adjusting the learning rate dynamically, 
                        the model is encouraged to converge better and avoid overfitting.



-> Architecture
    => Hyperparameter optimization using Grid Search and Heat Maps for:
        - batchsize: [32,64]
        - learning rate: [0.01, 0.0001]

    => Mini-batch training

    Model 1: ResNet50
     -> The base model ResNet50 is loaded with pre-trained weights from ImageNet and without the top classification layer.
     -> Additional layers (global pooling, dense layers, and dropout) are added to adapt the model for your specific dataset with three classes.


    => Model 2 : Adam (Momentum based NN)
        -> Input:
            - Batch Size
            - Height of image
            - Width of image
            - three color channels (RGB)

            The neural network input consists of images resized to 224 × 224 pixels, normalized to a [0, 1] range, \
            with three color channels (RGB).

        -> loss function : categorical_crossentropy
        -> metric of evaluation : accuracy
        -> Activation function : 
            - ReLu us applied at all layers except the output layer
            - SoftMax applied at output layer

        -> Hyperparameters :
            -> Batch Size
            -> Learning rate
            -> Number of Filters in Conv2D Layers: 32, 64, 64 in the Conv2D layers specify the number of filters to 
                learn at each layer. These determine how many feature maps will be generated by each convolutional layer.

            -> Kernel Size: (3, 3) in the Conv2D layers specifies the size of the convolutional kernel. 
                This defines how many pixels the kernel will cover during the convolution operation.

            -> Pooling Size: ndicates the pooling size, which reduces the spatial dimensions of the feature maps by 
                taking the maximum value over a 2x2 window.

            -> Pooling (We are using Max pooling. Add explnation for max pooling): Pooling is a downsampling operation used in Convolutional Neural Networks (CNNs) 
            to reduce the spatial dimensions (height and width) of the input feature maps. This process helps decrease the number of parameters, reduce computation, 
            and control overfitting while retaining the important information from the features.
                    - Dimensionality Reduction
                    - Translation invariance
                    - Reduce of overfitting


        Two-Phase Training:
        Phase 1: Phase 1: In the initial phase, only the top (newly added) layers of the model are trained while keeping the base ResNet50 layers frozen. 
                This ensures that the model learns the new data representations without altering the robust, pre-trained features of ResNet50. 
                The goal is to adapt the new classification layers to the specific dataset.

        Phase 2: Phase 2: After the initial training, the base model's layers are unfrozen, allowing the entire network to be trainable. 
                The model is then recompiled with a lower learning rate, obtained from hyperparameter optimazation step. 
                This phase fine-tunes the entire network, enabling slight adjustments to the pre-trained features to better fit the dataset.

        The Convolutional Neural Network (CNN) begins with an input layer accepting RGB images of size 224x224x3 pixels. 
        The first convolutional layer applies 32 filters of size 3x3, followed by ReLU activation to introduce non-linearity, 
        and then a 2x2 max pooling layer that reduces the spatial dimensions by half. The second layer deepens the network with 
        64 3x3 filters, again followed by ReLU and another 2x2 max pooling operation. A third convolutional block with 64 more 3x3 filters, 
        ReLU activation, and max pooling further processes the features. After the convolutional sections, the network flattens the 3D feature maps 
        into a 1D vector, which feeds into a dense layer with 64 neurons and ReLU activation. To prevent overfitting, a dropout layer is added 
        with a 50% dropout rate, randomly deactivating half the neurons during training. Finally, the output layer consists of 3 neurons with softmax activation, 
        corresponding to your three classes, producing probability distributions that sum to 1. The model uses the Adam optimizer and 
        categorical crossentropy loss function, which is appropriate for multi-class classification tasks, while tracking accuracy as the performance metric.



-> Early stopping if the validation loss does not improve for 5 epochs


Analysis of Results:
	- final accuracy + charts